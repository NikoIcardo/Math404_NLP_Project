{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Me, messing around\n",
    "\n",
    "*Julia Piaskowski*    \n",
    "*2020-04-20*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: \n",
    "Following parts of this:\n",
    "\n",
    "https://www.kaggle.com/shivamb/beginners-guide-to-text-generation-using-lstms\n",
    "\n",
    "\n",
    "This is using NYT comments data set that consists of several CSV files for articles by selected months and comments.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data utility function\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string, os \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for nlp functions \n",
    "\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory for all data\n",
    "data_dir = '../data/mydata/nyt-comments/'\n",
    "# this dataset can be downloaded here:\n",
    "# https://www.kaggle.com/aashita/nyt-comments\n",
    "\n",
    "# First, Get Article Headlines\n",
    "all_headlines = []\n",
    "\n",
    "for filename in os.listdir(data_dir):\n",
    "    if 'Articles' in filename:\n",
    "        article_df = pd.read_csv(data_dir + filename)\n",
    "        all_headlines.extend(list(article_df.headline.values))\n",
    "        break\n",
    "\n",
    "all_headlines = [h for h in all_headlines if h != \"Unknown\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "831"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check it worked: \n",
    "len(all_headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean Text\n",
    "\n",
    "This function removes punction, converts everthing to lowercase, and simplies the encoding from UTF-8 to ASCII. It's really nice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['finding an expansive view  of a forgotten people in niger',\n",
       " 'and now  the dreaded trump curse',\n",
       " 'venezuelas descent into dictatorship',\n",
       " 'stain permeates basketball blue blood',\n",
       " 'taking things for granted',\n",
       " 'the caged beast awakens',\n",
       " 'an everunfolding story',\n",
       " 'oreilly thrives as settlements add up',\n",
       " 'mouse infestation',\n",
       " 'divide in gop now threatens trump tax plan']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(txt):\n",
    "    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n",
    "    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n",
    "    return txt \n",
    "\n",
    "corpus = [clean_text(x) for x in all_headlines]\n",
    "# check that it worked\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### Tokenise\n",
    "\n",
    "(now back in pytorch text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "tokens = [tokenizer(x) for x in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "831\n",
      "['finding', 'an', 'expansive', 'view', 'of', 'a', 'forgotten', 'people', 'in', 'niger']\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "print(tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was fun, but this is a pain. Next step is to generate n-grams of each headine and pad the sequences. It would also help if were deaing with symbolic integers and not actual words at this time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Keras: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "import keras.utils as ku "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[169, 17],\n",
       " [169, 17, 665],\n",
       " [169, 17, 665, 367],\n",
       " [169, 17, 665, 367, 4],\n",
       " [169, 17, 665, 367, 4, 2],\n",
       " [169, 17, 665, 367, 4, 2, 666],\n",
       " [169, 17, 665, 367, 4, 2, 666, 170],\n",
       " [169, 17, 665, 367, 4, 2, 666, 170, 5],\n",
       " [169, 17, 665, 367, 4, 2, 666, 170, 5, 667],\n",
       " [6, 80]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this one is clearer, easier, faster:\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "def get_sequence_of_tokens(corpus):\n",
    "    ## tokenization\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    total_words = len(tokenizer.word_index) + 1\n",
    "    \n",
    "    ## convert data to sequence of tokens \n",
    "    input_sequences = []\n",
    "    for line in corpus:\n",
    "        token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "        for i in range(1, len(token_list)):\n",
    "            n_gram_sequence = token_list[:i+1]\n",
    "            input_sequences.append(n_gram_sequence)\n",
    "    return input_sequences, total_words\n",
    "\n",
    "inp_sequences, total_words = get_sequence_of_tokens(corpus)\n",
    "inp_sequences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4806\n"
     ]
    }
   ],
   "source": [
    "print(len(inp_sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have a collection of sequences broke in to $1, 2,,,,k_i$ n-grams (where $k_i$ is the length of each line of tokens from a single headline.\n",
    "\n",
    "All these sequences need to be padded to the length of the longest sequence (the maximum of $k_i$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padded_sequences(input_sequences):\n",
    "    max_sequence_len = max([len(x) for x in input_sequences])\n",
    "    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "    \n",
    "    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "    label = ku.to_categorical(label, num_classes=total_words)\n",
    "    return predictors, label, max_sequence_len\n",
    "\n",
    "predictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4806 4806\n",
      "19\n",
      "18\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 169]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 169  17]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 169  17 665]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0 169  17 665 367   4]\n"
     ]
    }
   ],
   "source": [
    "print(len(label), len(predictors))\n",
    "print(max_sequence_len)\n",
    "print(len(predictors[4]))\n",
    "print(predictors[0])\n",
    "print(predictors[1])\n",
    "print(predictors[2])\n",
    "print(predictors[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2422\n",
      "[0. 1.]\n",
      "[0. 0. 1. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(len(label[4]))  # length is consistent 2422 - the number of unique words\n",
    "print(np.unique(label)) \n",
    "    # just a yes/no for the next word in the sequence from the predictor matrix\n",
    "print(label[4]) # mostly zeros, of course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Model\n",
    "\n",
    "This is largely from the example, with a change to the optimizer (gradient clipping added).\n",
    "\n",
    "Model is to predict next word in sequence based on a collection words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_sequence_len, total_words):\n",
    "    input_len = max_sequence_len - 1\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add Input Embedding Layer\n",
    "        # I'm a little unclear on this. It's a sparse matrix solution. \n",
    "    model.add(Embedding(total_words, 10, input_length=input_len))\n",
    "    \n",
    "    # Add Hidden Layer 1, a LSTM Layer w/100 nodes\n",
    "    model.add(LSTM(100))\n",
    "    model.add(Dropout(0.1)) # and a dropout function\n",
    "    \n",
    "    # Add Output Layer\n",
    "    model.add(Dense(total_words, activation='softmax'))\n",
    "    \n",
    "    # optimiser\n",
    "    Adam = keras.optimizers.Adam(learning_rate=0.001, clipnorm = 1)\n",
    "\n",
    "    # add it all together\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model(max_sequence_len, total_words)\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1529de438>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(predictors, label, epochs=100, verbose=0)\n",
    "# default batch size is 32 \n",
    "# verbose = 0 is for silent\n",
    "# shuffles by default\n",
    "# loads of other parameters: https://keras.io/models/model/#fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assess model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4806/4806 [==============================] - 1s 118us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.3524951553820967"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here's our total loss:  \n",
    "# (using loss function defined in model)\n",
    "model.evaluate(predictors, label, batch_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "73.41"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# % correct\n",
    "preds = model.predict(predictors)\n",
    "# print(np.argmax(preds[0]))\n",
    "# print(np.argmax(label[0]))\n",
    "\n",
    "correct = 0\n",
    "\n",
    "for i,j in zip(preds, label):\n",
    "    if np.argmax(i) == np.argmax(j):\n",
    "        correct += 1\n",
    "\n",
    "np.round(correct/len(preds) * 100, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Generation \n",
    "\n",
    "(the good stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "        \n",
    "        output_word = \"\"\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idaho Days Of The Bluff The\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(\"idaho\", 5, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idaho Days Of The Bluff The Limits Of Trumps Negotiation Strategy Apart Niger\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(\"idaho\",12, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India A Song Of Face Creams\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(\"india\", 5, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall Street Bannon Carrier Moment From Statins\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(\"wall street\", 5, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Climate Change Became Ms On The Delta Used Of\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(\"climate change\", 7, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scientist Days Of The Bluff The Limits Of\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(\"scientist\", 7, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Days Of The Bluff The Limits Of\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(\"apple\", 7, model, max_sequence_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
